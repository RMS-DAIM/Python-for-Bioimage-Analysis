{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Task E\n",
    "\n",
    "**Part of IAFIG-RMS *Python for Bioimage Analysis* Course.**\n",
    "\n",
    "*M Kundegorski*\n",
    "\n",
    "2019-12-13\n",
    "\n",
    "In this task you will use image information to cluster images into classes based on similarity of colour. In reality we could use any feature but colour is very visual and easy to follow.\n",
    "\n",
    "There are 6 'classes' (colours) of blob, each of them has different value of hue (but the same mean intensity).\n",
    "\n",
    "This task is based on a scikit-learn tutorial of k-means clustering: https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html#sphx-glr-auto-examples-cluster-plot-cluster-iris-py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task\n",
    "\n",
    "In one experiment we were able to take a 'branbow'-esque dataset, i.e. every cell is expressing a combination of different fluorophores (colours), and each colour combination represents a type of cell. We imaged these cells with an RGB camera. These cells are sparse so we were easily able to segment them, find their bounding box and create a database of images each containing a single cell. We want to use clustering to automatically identify which cell belongs to which cell-type. Luckily, we've convinced a PhD student to manually label our sample database so we can confirm our results at the end.\n",
    "\n",
    "Given the colour of our cells we want to automatically, and in an unsupervised fashion, 'cluster' our cells into classes, ignoring other features like shape or size.\n",
    "\n",
    "1. Let's start by loading our data and visualising the manual class labels in RGB space (other colour spaces would also work)\n",
    "2. Use k-means clustering to cluster our data and compare it to our manual labels\n",
    "3. Advanced: Use sum of square errors to identify the correct 'k' to use\n",
    "4. Advanced: Use mean shift clustering to cluster our data and compare it to our manual labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task E.1\n",
    "\n",
    "Let's start by loading data. Later on, you can change parameters to see how the clustering results change with different numbers and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Utils is a custom module written to simplify these tutorials\n",
    "# You do not need to understand these codes for this practical\n",
    "from utils.practice_data import generateBlobsData  # this loads data into a DataFrame\n",
    "from utils.practice_data import showBlobs  # this allows quick visualisation of the data\n",
    "\n",
    "imageDir = './assets/simple_blobs/'\n",
    "number_of_classes = 6\n",
    "# Is 30 a sufficent number of samples? How much noise can our algorithms deal with?\n",
    "number_of_samples = 30\n",
    "noise = 10 #value from 0 to 250\n",
    "problem = generateBlobsData(imageDir, number_of_classes, number_of_samples, imSize = 100, colour=True, noiseSize = noise)\n",
    "\n",
    "# Visualise\n",
    "display(problem.loc[:,'class'].describe())  # describe classes\n",
    "showBlobs(problem.sample(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task E.2\n",
    "\n",
    "Time for some data wrangling - as usual the data needs to have right datatype required by the algorithm. For this simple clustering example we're going to extract the mean 'red', the mean 'green' and the mean 'blue' value for each cell. To make things simply, given we've only got one cell per image, we will use the means for the whole image. How might this affect our means and therefore our clustering? Should we use the mean or the median?\n",
    "\n",
    "Run the following cell to get the median colour channel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=problem.loc[:,'class'].values.astype(int)  # Convert classes to int\n",
    "\n",
    "for cell in problem.index:\n",
    "    problem.loc[cell,'mean-R'] = problem.loc[cell,'raw_data'][:,:,0].mean()  # mean of the first (red) channel of our image data for this cell\n",
    "    problem.loc[cell,'mean-G'] = problem.loc[cell,'raw_data'][:,:,1].mean()  # mean of the second (green) channel of our image data for this cell\n",
    "    problem.loc[cell,'mean-B'] = problem.loc[cell,'raw_data'][:,:,2].mean()  # mean of the third (blue) channel of our image data for this cell\n",
    "\n",
    "problem.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to plot our cells on three axes - one for R, one for G and one for B. This code also colourcodes cells by their class (the manual class at the moment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axis = plt.subplots(1,1,subplot_kw={'projection':'3d'})  # create a figure with a single 3d axis (subplot)\n",
    "\n",
    "# Get a list of unique classes\n",
    "blob_classes = problem.loc[:,'class'].unique()\n",
    "\n",
    "# Plot each blob and label\n",
    "for blob_class in blob_classes:\n",
    "    axis.plot(problem[problem['class']==blob_class]['mean-R'],\n",
    "            problem[problem['class']==blob_class]['mean-G'],\n",
    "            problem[problem['class']==blob_class]['mean-B'], \n",
    "            'o', label=blob_class)\n",
    "axis.set_ylabel('Red')\n",
    "axis.set_xlabel('Green')\n",
    "axis.set_zlabel('Blue')\n",
    "\n",
    "axis.legend(loc='upper left', numpoints=1, ncol=3, fontsize=8, bbox_to_anchor=(0, 0))  # add a legend\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task E.3\n",
    "\n",
    "Now you will use [sklearn.cluster.kMeans()](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) to perform k-means clustering on your data with k=6. This clustering algorithm never sees your manual class labels. Let's see if we can put your PhD student out of a job - I'm sure you have plenty more for them to do!\n",
    "\n",
    "Run the next cell to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "# Initialise clustering algorithm\n",
    "k_means = cluster.KMeans(n_clusters=6)  # How would you decide how many clusters to choose?\n",
    "\n",
    "# Perform fit with colour data only (no class labels)\n",
    "k_means.fit(problem[['mean-R','mean-G','mean-B']])\n",
    "\n",
    "# Extract cluster labels\n",
    "problem['kMeans-cluster'] = k_means.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to plot your PhD student's clustering and the k-means clustering side-by-side. Note that the cluster *numbers* may not map exactly but the cluster members should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,2,subplot_kw={'projection':'3d'})  # create a figure with two 3d axes (subplot)\n",
    "(axPhD, axKM) = ax.flatten()  # give each subplot a unique name\n",
    "\n",
    "# Get a list of unique PhD-student-derived classes\n",
    "phd_classes = problem.loc[:,'class'].unique()\n",
    "\n",
    "# Plot each blob and label\n",
    "for blob_class in blob_classes:\n",
    "    axPhD.plot(problem[problem['class']==blob_class]['mean-R'],\n",
    "            problem[problem['class']==blob_class]['mean-G'],\n",
    "            problem[problem['class']==blob_class]['mean-B'], \n",
    "            'o', label=blob_class)\n",
    "axPhD.set_ylabel('Red')\n",
    "axPhD.set_xlabel('Green')\n",
    "axPhD.set_zlabel('Blue')\n",
    "\n",
    "axPhD.legend(loc='upper left', numpoints=1, ncol=3, fontsize=8, bbox_to_anchor=(0, 0))  # add a legend\n",
    "\n",
    "# Get a list of unique k-Means-derived classes\n",
    "km_classes = problem.loc[:,'kMeans-cluster'].unique()\n",
    "\n",
    "# Plot each blob and label\n",
    "for blob_class in blob_classes:\n",
    "    axKM.plot(problem[problem['class']==blob_class]['mean-R'],\n",
    "            problem[problem['class']==blob_class]['mean-G'],\n",
    "            problem[problem['class']==blob_class]['mean-B'], \n",
    "            'o', label=blob_class)\n",
    "axKM.set_ylabel('Red')\n",
    "axKM.set_xlabel('Green')\n",
    "axKM.set_zlabel('Blue')\n",
    "\n",
    "axKM.legend(loc='upper left', numpoints=1, ncol=3, fontsize=8, bbox_to_anchor=(0, 0))  # add a legend\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Task E.4\n",
    "\n",
    "So far, we used our senses to guess that we needed 6 clusters. However, there is a relatively simple way to estimate the number of clusters we want from a k-means algorithm. First, we guess a reasonable range, for example, 1 to 10 clusters, and run k-means clustering for each guess. We then look for a 'knee' point in Sum of Square Errors (SSE) for each clustering. The SSE (or intertia) is the sum of distances from each sample (each cell) to the centre of it's cluster, e.g. the mean position of all cells in that cluster.\n",
    "\n",
    "You can access value of SSE using `.inertia_` attribute of a fitted k-means model.\n",
    "\n",
    "Use a for loop, and by copying the code above, create a cell to first the best 'k' to use for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove this cell ###\n",
    "sse = np.zeros(9)  # create an array to hold our sse values\n",
    "\n",
    "for k in np.arange(1, 10):\n",
    "    # Initialise clustering algorithm\n",
    "    k_means = cluster.KMeans(n_clusters=k)  # How would you decide how many clusters to choose?\n",
    "\n",
    "    # Perform fit with colour data only (no class labels)\n",
    "    k_means.fit(problem[['mean-R','mean-G','mean-B']])\n",
    "\n",
    "    # Extract inertia\n",
    "    sse[k] = kmeans.inertia_\n",
    "\n",
    "# Plot SSE for different k\n",
    "f, axis = plt.subplots(1,1)  # create a figure with a single axis (subplot)\n",
    "\n",
    "axis.plot(np.arange(1,10), sse)\n",
    "axis.set_ylabel('SSE')\n",
    "axis.set_xlabel('Number of Clusters (k)')\n",
    "\n",
    "plt.show()\n",
    "### Remove this cell ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Shift Clustering: What happens if we change `bandwidth` parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "ms = MeanShift(bandwidth=10).fit(problem[['ch1','ch2','ch3']])\n",
    "problem['cluster_ms'] = ms.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare kmeans and Mean Shift clustering with ground-truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show cluster\n",
    "fig = plt.figure(figsize=plt.figaspect(0.3))\n",
    "ax1 = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "ax2 = fig.add_subplot(1, 3, 2, projection='3d')\n",
    "ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "\n",
    "blob_classes = problem['class'].unique()\n",
    "for blob_class in blob_classes:\n",
    "    ax1.plot(problem[problem['class']==blob_class]['ch1'],\n",
    "            problem[problem['class']==blob_class]['ch2'],\n",
    "            problem[problem['class']==blob_class]['ch3'], \n",
    "            'o', label=blob_class)\n",
    "ax1.set_title('Ground truth label')    \n",
    "blob_classes = problem['cluster'].unique()\n",
    "for blob_class in blob_classes:\n",
    "    ax2.plot(problem[problem['cluster']==blob_class]['ch1'],\n",
    "            problem[problem['cluster']==blob_class]['ch2'],\n",
    "            problem[problem['cluster']==blob_class]['ch3'], \n",
    "            'o', label=blob_class)\n",
    "ax2.set_title('k-means with {} cluster'.format(len(blob_classes)))\n",
    "    \n",
    "blob_classes = problem['cluster_ms'].unique()\n",
    "for blob_class in blob_classes:\n",
    "    ax3.plot(problem[problem['cluster_ms']==blob_class]['ch1'],\n",
    "            problem[problem['cluster_ms']==blob_class]['ch2'],\n",
    "            problem[problem['cluster_ms']==blob_class]['ch3'], \n",
    "            'o', label=blob_class)\n",
    "ax3.set_title('Mean shift clustering with {} clusters'.format(len(blob_classes)))    \n",
    "    \n",
    "ax1.legend(loc='upper left', numpoints=1, ncol=3, fontsize=8, bbox_to_anchor=(0, 0))\n",
    "ax2.legend(loc='upper left', numpoints=1, ncol=3, fontsize=8, bbox_to_anchor=(0, 0))\n",
    "ax3.legend(loc='upper left', numpoints=1, ncol=3, fontsize=8, bbox_to_anchor=(0, 0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
