{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Task F\n",
    "\n",
    "**Part of IAFIG-RMS *Python for Bioimage Analysis* Course.**\n",
    "\n",
    "*M Kundegorski*\n",
    "\n",
    "2019-12-13\n",
    "\n",
    "In this task you will classify toy data of 'blobs' using Histogram of Gradient (HoG) features and two classifiers: Support Vector Machines (SVM) and Random Forests.\n",
    "\n",
    "We will use the same data from the logistic regression we did in Task B so you can compare the quality of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task (c.f. Task B)\n",
    "\n",
    "In an experiment we were able to image lots of different types of cells. These cells are sparse so we were easily able to segment them, find their bounding box and create a database of images each containing a single cell. We then convinced a PhD student to go through and manually categorise our cells. We want to use regression to be able to automatically categorise new, unlabelled cells from future experiments.\n",
    "\n",
    "To do this, we will:\n",
    "1. Calculate Histogram of Gradient (HoG) features for our images.\n",
    "2. Use Support Vector Machines (SVM) for classification.\n",
    "3. Use training data to fit the regression and test data to check how well our model works.\n",
    "4. Plot and understand a confusion matrix for classification.\n",
    "5. Advanced: Use visual inspection to understand what might cause problems for classification models.\n",
    "6. Advanced: Repeat using Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task F.1\n",
    "\n",
    "Run the following two cells to set-up and visualise the data. Feel free to change parameters as you explore the system. We suggest you use the same settings as for Task B so that you can compare the different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2  # OpenCV - we will use their HoG feature descriptor and their SVM\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Utils is a custom module written to simplify these tutorials\n",
    "# You do not need to understand these codes for this practical\n",
    "from utils.practice_data import generateBlobsData  # this loads data into a DataFrame\n",
    "from utils.practice_data import showBlobs  # this allows quick visualisation of the data\n",
    "\n",
    "# Generate a pandas DataFrame of data\n",
    "# with a column 'class', i.e. the categry a cell belongs to,\n",
    "# and a column 'raw_data' which hold the NumPy array/image\n",
    "imageDir = './assets/simple_blobs/'\n",
    "number_of_samples = 1200\n",
    "image_size = 64  # in pixels\n",
    "number_of_classes = 3 #2-6 - normal blobs. 7-11 more difficult\n",
    "problem = generateBlobsData(imageDir, number_of_classes, number_of_samples, image_size, noiseSize=20)\n",
    "\n",
    "#Visualise the data\n",
    "display(problem.loc[:,'class'].describe())  # describe classes; note the number of unique classes\n",
    "showBlobs(problem.sample(10))  # plots the images with their class above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task F.2\n",
    "\n",
    "First we need to 'wrangle' our data into appropriate forms. Run the cell below to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data wrangling\n",
    "x = np.stack(problem.loc[:,'raw_data'])  # Stack all images into an array 'x'\n",
    "y = problem.loc[:,'class'].values.astype(int)  # Convert classes to int\n",
    "\n",
    "# Split data into 80% training and 20% testing data\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell calculates and displace HoG descriptors. Run the cell. Don't worry to much about the details now but this tutorial will help you understand later: https://www.learnopencv.com/histogram-of-oriented-gradients/. These features are what we'll use for classification during this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this task we will use HoG descriptor from OpenCV.\n",
    "# You can find horrible documentation at https://docs.opencv.org/4.1.2/d5/d33/structcv_1_1HOGDescriptor.html#a5c8e8ce0578512fe80493ed3ed88ca83\n",
    "\n",
    "# HoG parameters\n",
    "winSize = (32,32)\n",
    "blockSize = (8,8)\n",
    "blockStride = (8,8)\n",
    "cellSize = (4,4)\n",
    "bins = 9\n",
    "\n",
    "# Initialise HoD Descriptor\n",
    "# This creates a weird HoG object\n",
    "# We will use this to calculate descriptors for individual images\n",
    "hog_cv = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, bins)\n",
    "\n",
    "# Extract HoG Features for Each Cell\n",
    "x_hog = []\n",
    "for cell in x:\n",
    "    descriptor = hog_cv.compute(cell)  # calculate descriptor for cell\n",
    "    x_hog.append(descriptor)  # append to our list\n",
    "\n",
    "x_hog = np.squeeze(np.array(x_hog))  # a little shape wrangling (drops a singleton dimension left by the descriptor)\n",
    "\n",
    "# Split data into 80% training and 20% testing data\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x_hog, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task F.3\n",
    "\n",
    "The following code cell constructs and trains a [scikit learn SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) for classifying our cells in our images.\n",
    "\n",
    "Fill in the blanks (`____`) and run the cell.\n",
    "\n",
    "Note the similarity to the code for Task B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "# Initialise SVM\n",
    "svm = svm.SVC()\n",
    "\n",
    "# 'Fit' with training data\n",
    "svm.fit(____, ____)\n",
    "\n",
    "# Predict the classes for our test data\n",
    "y_predict = svm.predict(____)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(____, ____))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task F.4\n",
    "\n",
    "Using your codes from Task B, plot the True/False rates for our SVM prediction. (When only considering two categories.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task F.5\n",
    "\n",
    "The bar chart above is a great way to see if a classifier is working well with only a few groups, but another way of representing the success of your classified is a 'confusion matrix' - and this will work much better with larger numbers of classes.\n",
    "\n",
    "Run the following cell to create a confusion matrix for your SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axis = plt.subplots(1,1)  # create a figure with a single axis (subplot)\n",
    "\n",
    "metrics.plot_confusion_matrix(svm, x_test, y_test, cmap=plt.cm.Blues, ax=axis)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Task F.6\n",
    "\n",
    "Change the number of cell classes in our training data from 2 to a number between 2 and 6 (inclusive). Rerun the HoG and SVM (all cells above), how does the model deal with more classes.\n",
    "\n",
    "Repeat for a number between 7 and 11 (inclusive). Look at the plotted sample data, can you see why classifiers may not be able to distinguish certain categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Task F.7\n",
    "\n",
    "Run the cells below to see the same experiment done using [scikit-learn Random Forests](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest#sklearn.ensemble.RandomForestClassifier).\n",
    "\n",
    "Add cells plotting the True/False rate and confusion matrix for random forest classification.\n",
    "\n",
    "Repeat task F.6 for Random Forests. Which performs better - SVM or Random Forests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "# Initialise Random Forest\n",
    "rf = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# 'Fit' with training data\n",
    "rf.fit(____, ____)\n",
    "\n",
    "# Predict the classes for our test data\n",
    "y_predict = rf.predict(____)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(____, ____))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
